<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="torchdrug.models" href="models.html" /><link rel="prev" title="torchdrug.metrics" href="metrics.html" />

    <meta name="generator" content="sphinx-3.1.2, furo 2020.12.09.beta21"/>
        <title>torchdrug.layers - TorchDrug 0.1.0 documentation</title>
      <link rel="stylesheet" href="../_static/styles/furo.css?digest=a3f371badb8538d75df213ccffb17a4f6e8f3ac5">
    <link rel="stylesheet" href="../_static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="../_static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --font-stack: Helvetica, Arial, sans-serif, -apple-system;
  --font-stack--monospace: Courier, monospace;
  --color-brand-primary: #E5261F;
  --color-brand-content: #E5261F;
  --admonition-font-size: 1rem;
  --admonition-title-font-size: 1rem;
  --font-size--small--2: var(--font-size--small);
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --font-stack: Helvetica, Arial, sans-serif, -apple-system;
  --font-stack--monospace: Courier, monospace;
  --color-brand-primary: #E5261F;
  --color-brand-content: #E5261F;
  --admonition-font-size: 1rem;
  --admonition-title-font-size: 1rem;
  --font-size--small--2: var(--font-size--small);
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link id="pygments_dark_css" media="(prefers-color-scheme: dark)" rel="stylesheet" type="text/css" href="../_static/pygments_dark.css" />
    <link rel="stylesheet" href="../_static/styles/furo-extensions.css?digest=26485485040e7aaf717c13fd0188a5ad2c2deb60">
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script defer src="../_static/jquery.js"></script>
    <script defer src="../_static/underscore.js"></script>
    <script defer src="../_static/doctools.js"></script>
    <script defer src="../_static/language_data.js"></script>
    <script defer async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script defer src="../_static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>


<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">TorchDrug 0.1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="https://torchdrug.ai">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.svg" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html">
  <input class="sidebar-search" placeholder="Search" name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">Quick Start</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../benchmark/index.html">Benchmark</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label for="toctree-checkbox-1"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../benchmark/property_prediction.html">Molecule Property Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../benchmark/pretrain.html">Pretrained Molecular Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../benchmark/generation.html">Molecule Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../benchmark/retrosynthesis.html">Retrosynthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../benchmark/reasoning.html">Knowledge Graph Reasoning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials/index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label for="toctree-checkbox-2"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/property_prediction.html">Property Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/pretrain.html">Pretrained Molecular Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/generation.html">Molecule Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/retrosynthesis.html">Retrosynthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/reasoning.html">Knowledge Graph Reasoning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../notes/index.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label for="toctree-checkbox-3"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../notes/graph.html">Graph Data Structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/variadic.html">Batch Irregular Structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/layer.html">Graph Neural Network Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/model.html">Customize Models &amp; Tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../paper.html">Papers Implemented</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Documentation</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label for="toctree-checkbox-4"><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="core.html">torchdrug.core</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">torchdrug.data</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">torchdrug.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html">torchdrug.transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">torchdrug.metrics</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">torchdrug.layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html">torchdrug.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="tasks.html">torchdrug.tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">torchdrug.utils</a></li>
</ul>
</li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <div class="section" id="torchdrug-layers">
<h1>torchdrug.layers<a class="headerlink" href="#torchdrug-layers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="common-layers">
<h2>Common Layers<a class="headerlink" href="#common-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="gaussiansmearing">
<h3>GaussianSmearing<a class="headerlink" href="#gaussiansmearing" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.GaussianSmearing">
<em class="property">class </em><code class="sig-name descname">GaussianSmearing</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">stop</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">num_kernel</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">centered</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">learnable</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#GaussianSmearing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GaussianSmearing" title="Permalink to this definition">¶</a></dt>
<dd><p>Gaussian smearing from
<a class="reference external" href="https://arxiv.org/pdf/1706.08566.pdf">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</a>.</p>
<p>There are two modes for Gaussian smearing.</p>
<p>Non-centered mode:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\mu = [0, 1, ..., n], \sigma = [1, 1, ..., 1]\]</div></div>
<p>Centered mode:</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\mu = [0, 0, ..., 0], \sigma = [0, 1, ..., n]\]</div></div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<em>int</em><em>, </em><em>optional</em>) – minimal input value</p></li>
<li><p><strong>stop</strong> (<em>int</em><em>, </em><em>optional</em>) – maximal input value</p></li>
<li><p><strong>num_kernel</strong> (<em>int</em><em>, </em><em>optional</em>) – number of RBF kernels</p></li>
<li><p><strong>centered</strong> (<em>bool</em><em>, </em><em>optional</em>) – centered mode or not</p></li>
<li><p><strong>learnable</strong> (<em>bool</em><em>, </em><em>optional</em>) – learnable gaussian parameters or not</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.GaussianSmearing.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#GaussianSmearing.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GaussianSmearing.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute smeared gaussian features between data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – data of shape <span class="math notranslate nohighlight">\((..., d)\)</span></p></li>
<li><p><strong>y</strong> (<em>Tensor</em>) – data of shape <span class="math notranslate nohighlight">\((..., d)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>features of shape <span class="math notranslate nohighlight">\((..., num\_kernel)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="multilayerperceptron">
<h3>MultiLayerPerceptron<a class="headerlink" href="#multilayerperceptron" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MultiLayerPerceptron">
<em class="property">class </em><code class="sig-name descname">MultiLayerPerceptron</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">hidden_dims</span></em>, <em class="sig-param"><span class="n">short_cut</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em>, <em class="sig-param"><span class="n">dropout</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#MultiLayerPerceptron"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MultiLayerPerceptron" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-layer Perceptron.</p>
<p>Note there is no batch normalization, activation or dropout in the last layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>hidden_dim</strong> (<em>list of int</em>) – hidden dimensions</p></li>
<li><p><strong>short_cut</strong> (<em>bool</em><em>, </em><em>optional</em>) – use short cut or not</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – dropout rate</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="mutualinformation">
<h3>MutualInformation<a class="headerlink" href="#mutualinformation" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MutualInformation">
<em class="property">class </em><code class="sig-name descname">MutualInformation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">num_mlp_layer</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#MutualInformation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MutualInformation" title="Permalink to this definition">¶</a></dt>
<dd><p>Mutual information estimator from
<a class="reference external" href="https://arxiv.org/pdf/1808.06670.pdf">Learning deep representations by mutual information estimation and maximization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>num_mlp_layer</strong> (<em>int</em><em>, </em><em>optional</em>) – number of MLP layers</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="pairnorm">
<h3>PairNorm<a class="headerlink" href="#pairnorm" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.PairNorm">
<em class="property">class </em><code class="sig-name descname">PairNorm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">scale_individual</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#PairNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.PairNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Pair normalization layer proposed in <a class="reference external" href="https://openreview.net/pdf?id=rkecl1rtwB">PairNorm: Tackling Oversmoothing in GNNs</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale_individual</strong> (<em>bool</em><em>, </em><em>optional</em>) – additionally normalize each node representation to have the same L2-norm</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="sequential">
<h3>Sequential<a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.Sequential">
<em class="property">class </em><code class="sig-name descname">Sequential</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="n">global_args</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">allow_unused</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/common.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Improved sequential container.
Modules will be called in the order they are passed to the constructor.</p>
<p>Compared to the vanilla nn.Sequential, this layer additionally supports the following features.</p>
<ol class="arabic simple">
<li><p>Multiple input / output arguments.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer1 signature: (...) -&gt; (a, b)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 signature: (a, b) -&gt; (...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Global arguments.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer1 signature: (graph, a) -&gt; b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 signature: (graph, b) -&gt; c</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">,</span> <span class="n">global_args</span><span class="o">=</span><span class="p">(</span><span class="s2">"graph"</span><span class="p">,))</span>
</pre></div>
</div>
<p>Note the global arguments don’t need to be present in every layer.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer1 signature: (graph, a) -&gt; b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 signature: b -&gt; c</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer3 signature: (graph, c) -&gt; d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">,</span> <span class="n">global_args</span><span class="o">=</span><span class="p">(</span><span class="s2">"graph"</span><span class="p">,))</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Dict outputs.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer1 signature: a -&gt; {"b": b, "c": c}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 signature: b -&gt; d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>When dict outputs are used with global arguments, the global arguments can be explicitly
overwritten by any layer outputs.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer1 signature: (graph, a) -&gt; {"graph": graph, "b": b}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 signature: (graph, b) -&gt; c</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># layer2 takes in the graph output by layer1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">,</span> <span class="n">global_args</span><span class="o">=</span><span class="p">(</span><span class="s2">"graph"</span><span class="p">,))</span>
</pre></div>
</div>
</dd></dl>
</div>
</div>
<div class="section" id="convolution-layers">
<h2>Convolution Layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="torchdrug.layers.MessagePassingBase">
<em class="property">class </em><code class="sig-name descname">MessagePassingBase</code><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Base module for message passing.</p>
<p>Any custom message passing module should be derived from this class.</p>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassingBase.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassingBase.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassingBase.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform message passing over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassingBase.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassingBase.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassingBase.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassingBase.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<div class="section" id="chebyshevconv">
<h3>ChebyshevConv<a class="headerlink" href="#chebyshevconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.ChebyshevConv">
<em class="property">class </em><code class="sig-name descname">ChebyshevConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Chebyshev spectral graph convolution operator from
<a class="reference external" href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>k</strong> (<em>int</em><em>, </em><em>optional</em>) – number of Chebyshev polynomials.
This also corresponds to the radius of the receptive field.</p></li>
<li><p><strong>hidden_dims</strong> (<em>list of int</em><em>, </em><em>optional</em>) – hidden dims of edge network</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.ChebyshevConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ChebyshevConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ChebyshevConv.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform message passing over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ChebyshevConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ChebyshevConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ChebyshevConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ChebyshevConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="continuousfilterconv">
<h3>ContinuousFilterConv<a class="headerlink" href="#continuousfilterconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.ContinuousFilterConv">
<em class="property">class </em><code class="sig-name descname">ContinuousFilterConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cutoff</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">num_gaussian</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'shifted_softplus'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ContinuousFilterConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ContinuousFilterConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Continuous filter operator from
<a class="reference external" href="https://arxiv.org/pdf/1706.08566.pdf">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>hidden_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – hidden dimension. By default, same as <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_dim</span></code></p></li>
<li><p><strong>cutoff</strong> (<em>float</em><em>, </em><em>optional</em>) – maximal scale for RBF kernels</p></li>
<li><p><strong>num_gaussian</strong> (<em>int</em><em>, </em><em>optional</em>) – number of RBF kernels</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.ContinuousFilterConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ContinuousFilterConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ContinuousFilterConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ContinuousFilterConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ContinuousFilterConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ContinuousFilterConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ContinuousFilterConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ContinuousFilterConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ContinuousFilterConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ContinuousFilterConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#ContinuousFilterConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ContinuousFilterConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="graphattentionconv">
<h3>GraphAttentionConv<a class="headerlink" href="#graphattentionconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.GraphAttentionConv">
<em class="property">class </em><code class="sig-name descname">GraphAttentionConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_head</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">negative_slope</span><span class="o">=</span><span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">concat</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphAttentionConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphAttentionConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Graph attentional convolution operator from <a class="reference external" href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>num_head</strong> (<em>int</em><em>, </em><em>optional</em>) – number of attention heads</p></li>
<li><p><strong>negative_slope</strong> (<em>float</em><em>, </em><em>optional</em>) – negative slope of leaky relu activation</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphAttentionConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphAttentionConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphAttentionConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphAttentionConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphAttentionConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphAttentionConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphAttentionConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphAttentionConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphAttentionConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="graphconv">
<h3>GraphConv<a class="headerlink" href="#graphconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.GraphConv">
<em class="property">class </em><code class="sig-name descname">GraphConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Graph convolution operator from <a class="reference external" href="https://arxiv.org/pdf/1609.02907.pdf">Semi-Supervised Classification with Graph Convolutional Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="graphisomorphismconv">
<h3>GraphIsomorphismConv<a class="headerlink" href="#graphisomorphismconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.GraphIsomorphismConv">
<em class="property">class </em><code class="sig-name descname">GraphIsomorphismConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hidden_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">learn_eps</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphIsomorphismConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphIsomorphismConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Graph isomorphism convolution operator from <a class="reference external" href="https://arxiv.org/pdf/1810.00826.pdf">How Powerful are Graph Neural Networks?</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>hidden_dims</strong> (<em>list of int</em><em>, </em><em>optional</em>) – hidden dimensions</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – initial epsilon</p></li>
<li><p><strong>learn_eps</strong> (<em>bool</em><em>, </em><em>optional</em>) – learn epsilon or not</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphIsomorphismConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphIsomorphismConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphIsomorphismConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphIsomorphismConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphIsomorphismConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphIsomorphismConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphIsomorphismConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphIsomorphismConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphIsomorphismConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.GraphIsomorphismConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#GraphIsomorphismConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.GraphIsomorphismConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="messagepassing">
<h3>MessagePassing<a class="headerlink" href="#messagepassing" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MessagePassing">
<em class="property">class </em><code class="sig-name descname">MessagePassing</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span></em>, <em class="sig-param"><span class="n">hidden_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassing" title="Permalink to this definition">¶</a></dt>
<dd><p>Message passing operator from <a class="reference external" href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a>.</p>
<p>This implements the edge network variant in the original paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em>) – dimension of edge features</p></li>
<li><p><strong>hidden_dims</strong> (<em>list of int</em><em>, </em><em>optional</em>) – hidden dims of edge network</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassing.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassing.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassing.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassing.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassing.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassing.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.MessagePassing.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#MessagePassing.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MessagePassing.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="neuralfingerprintconv">
<h3>NeuralFingerprintConv<a class="headerlink" href="#neuralfingerprintconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.NeuralFingerprintConv">
<em class="property">class </em><code class="sig-name descname">NeuralFingerprintConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#NeuralFingerprintConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NeuralFingerprintConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Graph neural network operator from <a class="reference external" href="https://arxiv.org/pdf/1509.09292.pdf">Convolutional Networks on Graphs for Learning Molecular Fingerprints</a>.</p>
<p>Note this operator doesn’t include the sparsifying step of the original paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.NeuralFingerprintConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#NeuralFingerprintConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NeuralFingerprintConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.NeuralFingerprintConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#NeuralFingerprintConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NeuralFingerprintConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.NeuralFingerprintConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#NeuralFingerprintConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NeuralFingerprintConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.NeuralFingerprintConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#NeuralFingerprintConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NeuralFingerprintConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="relationalgraphconv">
<h3>RelationalGraphConv<a class="headerlink" href="#relationalgraphconv" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.RelationalGraphConv">
<em class="property">class </em><code class="sig-name descname">RelationalGraphConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">num_relation</span></em>, <em class="sig-param"><span class="n">edge_input_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_norm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#RelationalGraphConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.RelationalGraphConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Relational graph convolution operator from <a class="reference external" href="https://arxiv.org/pdf/1703.06103.pdf">Modeling Relational Data with Graph Convolutional Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – output dimension</p></li>
<li><p><strong>num_relation</strong> (<em>int</em>) – number of relations</p></li>
<li><p><strong>edge_input_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – dimension of edge features</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply batch normalization on nodes or not</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.RelationalGraphConv.aggregate">
<code class="sig-name descname">aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">message</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#RelationalGraphConv.aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.RelationalGraphConv.aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate edge messages to nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>message</strong> (<em>Tensor</em>) – edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.RelationalGraphConv.combine">
<code class="sig-name descname">combine</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">update</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#RelationalGraphConv.combine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.RelationalGraphConv.combine" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine node input and node update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
<li><p><strong>update</strong> (<em>Tensor</em>) – node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.RelationalGraphConv.message">
<code class="sig-name descname">message</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#RelationalGraphConv.message"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.RelationalGraphConv.message" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute edge messages for the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>edge messages of shape <span class="math notranslate nohighlight">\((|E|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.RelationalGraphConv.message_and_aggregate">
<code class="sig-name descname">message_and_aggregate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/conv.html#RelationalGraphConv.message_and_aggregate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.RelationalGraphConv.message_and_aggregate" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused computation of message and aggregation over the graph.
This may provide better time or memory complexity than separate calls of
<a class="reference internal" href="#torchdrug.layers.MessagePassingBase.message" title="torchdrug.layers.MessagePassingBase.message"><code class="xref py py-meth docutils literal notranslate"><span class="pre">message</span></code></a> and <a class="reference internal" href="#torchdrug.layers.MessagePassingBase.aggregate" title="torchdrug.layers.MessagePassingBase.aggregate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">aggregate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node updates of shape <span class="math notranslate nohighlight">\((|V|, ...)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="readout-layers">
<h2>Readout Layers<a class="headerlink" href="#readout-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="meanreadout">
<h3>MeanReadout<a class="headerlink" href="#meanreadout" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MeanReadout">
<em class="property">class </em><code class="sig-name descname">MeanReadout</code><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#MeanReadout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MeanReadout" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean readout operator over graphs with variadic sizes.</p>
<dl class="py method">
<dt id="torchdrug.layers.MeanReadout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#MeanReadout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MeanReadout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform readout over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>graph representations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="sumreadout">
<h3>SumReadout<a class="headerlink" href="#sumreadout" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.SumReadout">
<em class="property">class </em><code class="sig-name descname">SumReadout</code><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#SumReadout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.SumReadout" title="Permalink to this definition">¶</a></dt>
<dd><p>Sum readout operator over graphs with variadic sizes.</p>
<dl class="py method">
<dt id="torchdrug.layers.SumReadout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#SumReadout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.SumReadout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform readout over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>graph representations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="maxreadout">
<h3>MaxReadout<a class="headerlink" href="#maxreadout" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MaxReadout">
<em class="property">class </em><code class="sig-name descname">MaxReadout</code><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#MaxReadout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MaxReadout" title="Permalink to this definition">¶</a></dt>
<dd><p>Max readout operator over graphs with variadic sizes.</p>
<dl class="py method">
<dt id="torchdrug.layers.MaxReadout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#MaxReadout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MaxReadout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform readout over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>graph representations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="set2set">
<h3>Set2Set<a class="headerlink" href="#set2set" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.Set2Set">
<em class="property">class </em><code class="sig-name descname">Set2Set</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">num_step</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">num_lstm_layer</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Set2Set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Set2Set" title="Permalink to this definition">¶</a></dt>
<dd><p>Set2Set operator from <a class="reference external" href="https://arxiv.org/pdf/1511.06391.pdf">Order Matters: Sequence to sequence for sets</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>num_step</strong> (<em>int</em><em>, </em><em>optional</em>) – number of process steps</p></li>
<li><p><strong>num_lstm_layer</strong> (<em>int</em><em>, </em><em>optional</em>) – number of LSTM layers</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.Set2Set.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Set2Set.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Set2Set.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Set2Set readout over graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>graph representations</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.Softmax">
<em class="property">class </em><code class="sig-name descname">Softmax</code><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Softmax operator over graphs with variadic sizes.</p>
<dl class="py method">
<dt id="torchdrug.layers.Softmax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Softmax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Softmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform softmax over the graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node logits</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>node probabilities</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="sort">
<h3>Sort<a class="headerlink" href="#sort" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.Sort">
<em class="property">class </em><code class="sig-name descname">Sort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">descending</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Sort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Sort operator over graphs with variadic sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>descending</strong> (<em>bool</em><em>, </em><em>optional</em>) – use descending sort order or not</p>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.Sort.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/readout.html#Sort.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.Sort.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform sort over graph(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – node values</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>sorted values, sorted indices</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(Tensor, LongTensor)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="pooling-layers">
<h2>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="diffpool">
<h3>DiffPool<a class="headerlink" href="#diffpool" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.DiffPool">
<em class="property">class </em><code class="sig-name descname">DiffPool</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_node</span></em>, <em class="sig-param"><span class="n">feature_layer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_layer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">zero_diagonal</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">sparse</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/pool.html#DiffPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.DiffPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Differentiable pooling operator from <a class="reference external" href="https://papers.nips.cc/paper/7729-hierarchical-graph-representation-learning-with-differentiable-pooling.pdf">Hierarchical Graph Representation Learning with Differentiable Pooling</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_node</strong> (<em>int</em>) – number of nodes after pooling</p></li>
<li><p><strong>feature_layer</strong> (<em>Module</em><em>, </em><em>optional</em>) – graph convolution layer for embedding</p></li>
<li><p><strong>pool_layer</strong> (<em>Module</em><em>, </em><em>optional</em>) – graph convolution layer for pooling assignment</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em><em>, </em><em>optional</em>) – weight of entropy regularization</p></li>
<li><p><strong>zero_diagonal</strong> (<em>bool</em><em>, </em><em>optional</em>) – remove self loops in the pooled graph or not</p></li>
<li><p><strong>sparse</strong> (<em>bool</em><em>, </em><em>optional</em>) – use sparse assignment or not</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.DiffPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">all_loss</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/pool.html#DiffPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.DiffPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the node cluster assignment and pool the nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – input node representations</p></li>
<li><p><strong>all_loss</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – if specified, add loss to this tensor</p></li>
<li><p><strong>metric</strong> (<em>dict</em><em>, </em><em>optional</em>) – if specified, output metrics to this dict</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pooled graph, output node representations, node-to-cluster assignment</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="data.html#torchdrug.data.PackedGraph" title="torchdrug.data.PackedGraph">PackedGraph</a>, Tensor, Tensor)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mincutpool">
<h3>MinCutPool<a class="headerlink" href="#mincutpool" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.MinCutPool">
<em class="property">class </em><code class="sig-name descname">MinCutPool</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_node</span></em>, <em class="sig-param"><span class="n">feature_layer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_layer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">zero_diagonal</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">sparse</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/pool.html#MinCutPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MinCutPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Min cut pooling operator from <a class="reference external" href="http://proceedings.mlr.press/v119/bianchi20a/bianchi20a.pdf">Spectral Clustering with Graph Neural Networks for Graph Pooling</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension</p></li>
<li><p><strong>output_node</strong> (<em>int</em>) – number of nodes after pooling</p></li>
<li><p><strong>feature_layer</strong> (<em>Module</em><em>, </em><em>optional</em>) – graph convolution layer for embedding</p></li>
<li><p><strong>pool_layer</strong> (<em>Module</em><em>, </em><em>optional</em>) – graph convolution layer for pooling assignment</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em><em>, </em><em>optional</em>) – weight of entropy regularization</p></li>
<li><p><strong>zero_diagonal</strong> (<em>bool</em><em>, </em><em>optional</em>) – remove self loops in the pooled graph or not</p></li>
<li><p><strong>sparse</strong> (<em>bool</em><em>, </em><em>optional</em>) – use sparse assignment or not</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.MinCutPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">all_loss</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/pool.html#MinCutPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.MinCutPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the node cluster assignment and pool the nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – input node representations</p></li>
<li><p><strong>all_loss</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – if specified, add loss to this tensor</p></li>
<li><p><strong>metric</strong> (<em>dict</em><em>, </em><em>optional</em>) – if specified, output metrics to this dict</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pooled graph, output node representations, node-to-cluster assignment</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="data.html#torchdrug.data.PackedGraph" title="torchdrug.data.PackedGraph">PackedGraph</a>, Tensor, Tensor)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="sampler-layers">
<h2>Sampler Layers<a class="headerlink" href="#sampler-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="edgesampler">
<h3>EdgeSampler<a class="headerlink" href="#edgesampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.EdgeSampler">
<em class="property">class </em><code class="sig-name descname">EdgeSampler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ratio</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/sampler.html#EdgeSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.EdgeSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Edge sampler from <a class="reference external" href="https://arxiv.org/pdf/1907.04931.pdf">GraphSAINT: Graph Sampling Based Inductive Learning Method</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>budget</strong> (<em>int</em><em>, </em><em>optional</em>) – number of node to keep</p></li>
<li><p><strong>ratio</strong> (<em>int</em><em>, </em><em>optional</em>) – ratio of node to keep</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.EdgeSampler.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/sampler.html#EdgeSampler.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.EdgeSampler.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample a subgraph from the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="nodesampler">
<h3>NodeSampler<a class="headerlink" href="#nodesampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.NodeSampler">
<em class="property">class </em><code class="sig-name descname">NodeSampler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ratio</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/sampler.html#NodeSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NodeSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Node sampler from <a class="reference external" href="https://arxiv.org/pdf/1907.04931.pdf">GraphSAINT: Graph Sampling Based Inductive Learning Method</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>budget</strong> (<em>int</em><em>, </em><em>optional</em>) – number of node to keep</p></li>
<li><p><strong>ratio</strong> (<em>int</em><em>, </em><em>optional</em>) – ratio of node to keep</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.NodeSampler.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/sampler.html#NodeSampler.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.NodeSampler.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample a subgraph from the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>graph</strong> (<a class="reference internal" href="data.html#torchdrug.data.Graph" title="torchdrug.data.Graph"><em>Graph</em></a>) – graph(s)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="flow-layers">
<h2>Flow Layers<a class="headerlink" href="#flow-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conditionalflow">
<h3>ConditionalFlow<a class="headerlink" href="#conditionalflow" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.ConditionalFlow">
<em class="property">class </em><code class="sig-name descname">ConditionalFlow</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">condition_dim</span></em>, <em class="sig-param"><span class="n">hidden_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'relu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/flow.html#ConditionalFlow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ConditionalFlow" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditional flow transformation from <a class="reference external" href="https://arxiv.org/pdf/1705.07057.pdf">Masked Autoregressive Flow for Density Estimation</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input &amp; output dimension</p></li>
<li><p><strong>condition_dim</strong> (<em>int</em>) – condition dimension</p></li>
<li><p><strong>hidden_dims</strong> (<em>list of int</em><em>, </em><em>optional</em>) – hidden dimensions</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – activation function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.ConditionalFlow.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">condition</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/flow.html#ConditionalFlow.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ConditionalFlow.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform data into latent representations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input representations</p></li>
<li><p><strong>condition</strong> (<em>Tensor</em>) – conditional representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>latent representations, log-likelihood of the transformation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(Tensor, Tensor)</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.ConditionalFlow.reverse">
<code class="sig-name descname">reverse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">latent</span></em>, <em class="sig-param"><span class="n">condition</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/flow.html#ConditionalFlow.reverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.ConditionalFlow.reverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform latent representations into data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>latent</strong> (<em>Tensor</em>) – latent representations</p></li>
<li><p><strong>condition</strong> (<em>Tensor</em>) – conditional representations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>input representations, log-likelihood of the transformation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(Tensor, Tensor)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="distribution-layers">
<h2>Distribution Layers<a class="headerlink" href="#distribution-layers" title="Permalink to this headline">¶</a></h2>
<p>These layers belong to <cite>torchdrug.layers.distribution</cite>.</p>
<div class="section" id="independentgaussian">
<h3>IndependentGaussian<a class="headerlink" href="#independentgaussian" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="torchdrug.layers.distribution.IndependentGaussian">
<em class="property">class </em><code class="sig-name descname">IndependentGaussian</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mu</span></em>, <em class="sig-param"><span class="n">sigma2</span></em>, <em class="sig-param"><span class="n">learnable</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/distribution.html#IndependentGaussian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.distribution.IndependentGaussian" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent Gaussian distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mu</strong> (<em>Tensor</em>) – mean of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>sigma2</strong> (<em>Tensor</em>) – variance of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>learnable</strong> (<em>bool</em><em>, </em><em>optional</em>) – learnable parameters or not</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="torchdrug.layers.distribution.IndependentGaussian.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/distribution.html#IndependentGaussian.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.distribution.IndependentGaussian.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the likelihood of input data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<em>Tensor</em>) – input data of shape <span class="math notranslate nohighlight">\((..., N)\)</span></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="torchdrug.layers.distribution.IndependentGaussian.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/distribution.html#IndependentGaussian.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.distribution.IndependentGaussian.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from the distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>size</strong> (<em>tuple of int</em>) – shape of the samples</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
<div class="section" id="functional-layers">
<h2>Functional Layers<a class="headerlink" href="#functional-layers" title="Permalink to this headline">¶</a></h2>
<p>These layers belong to <cite>torchdrug.layers.functional</cite>.</p>
<div class="section" id="embedding-score-functions">
<h3>Embedding Score Functions<a class="headerlink" href="#embedding-score-functions" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.transe_score">
<code class="sig-name descname">transe_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">entity</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">h_index</span></em>, <em class="sig-param"><span class="n">t_index</span></em>, <em class="sig-param"><span class="n">r_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/embedding.html#transe_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.transe_score" title="Permalink to this definition">¶</a></dt>
<dd><p>TransE score function from <a class="reference external" href="https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf">Translating Embeddings for Modeling Multi-relational Data</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>entity</strong> (<em>Tensor</em>) – entity embeddings of shape <span class="math notranslate nohighlight">\((|V|, d)\)</span></p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – relation embeddings of shape <span class="math notranslate nohighlight">\((|R|, d)\)</span></p></li>
<li><p><strong>h_index</strong> (<em>LongTensor</em>) – index of head entities</p></li>
<li><p><strong>t_index</strong> (<em>LongTensor</em>) – index of tail entities</p></li>
<li><p><strong>r_index</strong> (<em>LongTensor</em>) – index of relations</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.distmult_score">
<code class="sig-name descname">distmult_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">entity</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">h_index</span></em>, <em class="sig-param"><span class="n">t_index</span></em>, <em class="sig-param"><span class="n">r_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/embedding.html#distmult_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.distmult_score" title="Permalink to this definition">¶</a></dt>
<dd><p>DistMult score function from <a class="reference external" href="https://arxiv.org/pdf/1412.6575.pdf">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>entity</strong> (<em>Tensor</em>) – entity embeddings of shape <span class="math notranslate nohighlight">\((|V|, d)\)</span></p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – relation embeddings of shape <span class="math notranslate nohighlight">\((|R|, d)\)</span></p></li>
<li><p><strong>h_index</strong> (<em>LongTensor</em>) – index of head entities</p></li>
<li><p><strong>t_index</strong> (<em>LongTensor</em>) – index of tail entities</p></li>
<li><p><strong>r_index</strong> (<em>LongTensor</em>) – index of relations</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.complex_score">
<code class="sig-name descname">complex_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">entity</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">h_index</span></em>, <em class="sig-param"><span class="n">t_index</span></em>, <em class="sig-param"><span class="n">r_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/embedding.html#complex_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.complex_score" title="Permalink to this definition">¶</a></dt>
<dd><p>ComplEx score function from <a class="reference external" href="http://proceedings.mlr.press/v48/trouillon16.pdf">Complex Embeddings for Simple Link Prediction</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>entity</strong> (<em>Tensor</em>) – entity embeddings of shape <span class="math notranslate nohighlight">\((|V|, 2d)\)</span></p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – relation embeddings of shape <span class="math notranslate nohighlight">\((|R|, 2d)\)</span></p></li>
<li><p><strong>h_index</strong> (<em>LongTensor</em>) – index of head entities</p></li>
<li><p><strong>t_index</strong> (<em>LongTensor</em>) – index of tail entities</p></li>
<li><p><strong>r_index</strong> (<em>LongTensor</em>) – index of relations</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.simple_score">
<code class="sig-name descname">simple_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">entity</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">h_index</span></em>, <em class="sig-param"><span class="n">t_index</span></em>, <em class="sig-param"><span class="n">r_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/embedding.html#simple_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.simple_score" title="Permalink to this definition">¶</a></dt>
<dd><p>SimplE score function from <a class="reference external" href="https://papers.nips.cc/paper/2018/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf">SimplE Embedding for Link Prediction in Knowledge Graphs</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>entity</strong> (<em>Tensor</em>) – entity embeddings of shape <span class="math notranslate nohighlight">\((|V|, 2d)\)</span></p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – relation embeddings of shape <span class="math notranslate nohighlight">\((|R|, d)\)</span></p></li>
<li><p><strong>h_index</strong> (<em>LongTensor</em>) – index of head entities</p></li>
<li><p><strong>t_index</strong> (<em>LongTensor</em>) – index of tail entities</p></li>
<li><p><strong>r_index</strong> (<em>LongTensor</em>) – index of relations</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.rotate_score">
<code class="sig-name descname">rotate_score</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">entity</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">h_index</span></em>, <em class="sig-param"><span class="n">t_index</span></em>, <em class="sig-param"><span class="n">r_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/embedding.html#rotate_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.rotate_score" title="Permalink to this definition">¶</a></dt>
<dd><p>RotatE score function from <a class="reference external" href="https://arxiv.org/pdf/1902.10197.pdf">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>entity</strong> (<em>Tensor</em>) – entity embeddings of shape <span class="math notranslate nohighlight">\((|V|, 2d)\)</span></p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – relation embeddings of shape <span class="math notranslate nohighlight">\((|R|, d)\)</span></p></li>
<li><p><strong>h_index</strong> (<em>LongTensor</em>) – index of head entities</p></li>
<li><p><strong>t_index</strong> (<em>LongTensor</em>) – index of tail entities</p></li>
<li><p><strong>r_index</strong> (<em>LongTensor</em>) – index of relations</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="sparse-matrix-multiplication">
<h3>Sparse Matrix Multiplication<a class="headerlink" href="#sparse-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.generalized_spmm">
<code class="sig-name descname">generalized_spmm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">sum</span><span class="o">=</span><span class="default_value">'add'</span></em>, <em class="sig-param"><span class="n">mul</span><span class="o">=</span><span class="default_value">'mul'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/spmm.html#generalized_spmm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.generalized_spmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Generalized sparse-dense matrix multiplication.</p>
<p>This function computes the matrix multiplication of a sparse matrix and a dense input matrix.
The output dense matrix satisfies</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output_{i,k} = \bigoplus_{j: sparse_{i,j} \neq 0} sparse_{i,j} \otimes input_{j,k}\]</div></div>
<p>where <span class="math notranslate nohighlight">\(\oplus\)</span> and <span class="math notranslate nohighlight">\(\otimes\)</span> are the summation and the multiplication operators respectively.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Gradient w.r.t. the sparse matrix is only computed for non-zero entries of the sparse matrix.
This behaves differently from dense-dense matrix multiplication with zero entries.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse</strong> (<em>SparseTensor</em>) – 2D sparse tensor</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – 2D dense tensor</p></li>
<li><p><strong>sum</strong> (<em>str</em><em>, </em><em>optional</em>) – generalized summation operator. Available operators are <code class="docutils literal notranslate"><span class="pre">add</span></code>, <code class="docutils literal notranslate"><span class="pre">min</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code>.</p></li>
<li><p><strong>mul</strong> (<em>str</em><em>, </em><em>optional</em>) – generalized multiplication operator. Available operators are <code class="docutils literal notranslate"><span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">mul</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.generalized_rspmm">
<code class="sig-name descname">generalized_rspmm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparse</span></em>, <em class="sig-param"><span class="n">relation</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">sum</span><span class="o">=</span><span class="default_value">'add'</span></em>, <em class="sig-param"><span class="n">mul</span><span class="o">=</span><span class="default_value">'mul'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/spmm.html#generalized_rspmm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.generalized_rspmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Generalized relational sparse-dense matrix multiplication.</p>
<p>This function computes the matrix multiplication of a sparse matrix, a dense relation matrix and
a dense input matrix. The output dense matrix satisfies</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[output_{i,l} = \bigoplus_{j,k: sparse_{i,j,k} \neq 0} sparse_{i, j, k} \times (relation_{k,l} \otimes input_{j,l})\]</div></div>
<p>where <span class="math notranslate nohighlight">\(\oplus\)</span> and <span class="math notranslate nohighlight">\(\otimes\)</span> are the summation and the multiplication operators respectively.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Gradient w.r.t. the sparse matrix is only computed for non-zero entries of the sparse matrix.
This behaves differently from dense-dense matrix multiplication with zero entries.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparse</strong> (<em>SparseTensor</em>) – 3D sparse tensor</p></li>
<li><p><strong>relation</strong> (<em>Tensor</em>) – 2D dense tensor</p></li>
<li><p><strong>input</strong> (<em>Tensor</em>) – 2D dense tensor</p></li>
<li><p><strong>sum</strong> (<em>str</em><em>, </em><em>optional</em>) – generalized summation operator. Available operators are <code class="docutils literal notranslate"><span class="pre">add</span></code>, <code class="docutils literal notranslate"><span class="pre">min</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code>.</p></li>
<li><p><strong>mul</strong> (<em>str</em><em>, </em><em>optional</em>) – generalized multiplication operator. Available operators are <code class="docutils literal notranslate"><span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">mul</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="variadic">
<h3>Variadic<a class="headerlink" href="#variadic" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_sum">
<code class="sig-name descname">variadic_sum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute sum over sets with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets, and the sizes of all sets are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>Tensor: sum</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_mean">
<code class="sig-name descname">variadic_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute mean over sets with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets, and the sizes of all sets are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>Tensor: mean</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_max">
<code class="sig-name descname">variadic_max</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_max" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute max over sets with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets, and the sizes of all sets are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>(Tensor, LongTensor): max values and indexes</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_cross_entropy">
<code class="sig-name descname">variadic_cross_entropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">target</span></em>, <em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cross entropy loss over categories with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> samples, and the numbers of categories in all samples are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – prediction of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>target</strong> (<em>Tensor</em>) – target of shape <span class="math notranslate nohighlight">\((N, ...)\)</span>. Each target is a relative index in a sample.</p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – number of categories of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – reduction to apply to the output.
Available reductions are <code class="docutils literal notranslate"><span class="pre">none</span></code>, <code class="docutils literal notranslate"><span class="pre">sum</span></code> and <code class="docutils literal notranslate"><span class="pre">mean</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_log_softmax">
<code class="sig-name descname">variadic_log_softmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute log softmax over categories with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> samples, and the numbers of categories in all samples are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – number of categories of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_sort">
<code class="sig-name descname">variadic_sort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">descending</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_sort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Sort elements in sets with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets, and the sizes of all sets are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>descending</strong> (<em>bool</em><em>, </em><em>optional</em>) – return ascending or descending order</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_topk">
<code class="sig-name descname">variadic_topk</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">largest</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_topk"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_topk" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the <span class="math notranslate nohighlight">\(k\)</span> largest elements over sets with variadic sizes.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets, and the sizes of all sets are summed to <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>If any set has less than than <span class="math notranslate nohighlight">\(k\)</span> elements, the size-th largest element will be
repeated to pad the output to <span class="math notranslate nohighlight">\(k\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input of shape <span class="math notranslate nohighlight">\((B, ...)\)</span></p></li>
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>k</strong> (<em>int</em>) – the k in “top-k”</p></li>
<li><p><strong>largest</strong> (<em>bool</em><em>, </em><em>optional</em>) – return largest or smallest elements</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>(Tensor, LongTensor): top-k values and indexes</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_arange">
<code class="sig-name descname">variadic_arange</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_arange"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_arange" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a 1-D tensor that contains integer intervals of variadic sizes.
This is a variadic variant of <code class="docutils literal notranslate"><span class="pre">torch.arange(stop).expand(batch_size,</span> <span class="pre">-1)</span></code>.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> intervals.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of intervals of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em>, </em><em>optional</em>) – device of the tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.variadic_randperm">
<code class="sig-name descname">variadic_randperm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">size</span></em>, <em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#variadic_randperm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.variadic_randperm" title="Permalink to this definition">¶</a></dt>
<dd><p>Return random permutations for sets with variadic sizes.
The <code class="docutils literal notranslate"><span class="pre">i</span></code>-th permutation contains integers from 0 to <code class="docutils literal notranslate"><span class="pre">size[i]</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p>
<p>Suppose there are <span class="math notranslate nohighlight">\(N\)</span> sets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>LongTensor</em>) – size of sets of shape <span class="math notranslate nohighlight">\((N,)\)</span></p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em>, </em><em>optional</em>) – device of the tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tensor-reduction">
<h3>Tensor Reduction<a class="headerlink" href="#tensor-reduction" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.masked_mean">
<code class="sig-name descname">masked_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdim</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#masked_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.masked_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Masked mean of a tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input tensor</p></li>
<li><p><strong>mask</strong> (<em>BoolTensor</em>) – mask tensor</p></li>
<li><p><strong>dim</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>optional</em>) – dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether retain <code class="docutils literal notranslate"><span class="pre">dim</span></code> or not</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.mean_with_nan">
<code class="sig-name descname">mean_with_nan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">keepdim</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#mean_with_nan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.mean_with_nan" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean of a tensor. Ignore all nan values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – input tensor</p></li>
<li><p><strong>dim</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>optional</em>) – dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether retain <code class="docutils literal notranslate"><span class="pre">dim</span></code> or not</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tensor-construction">
<h3>Tensor Construction<a class="headerlink" href="#tensor-construction" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.as_mask">
<code class="sig-name descname">as_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indexes</span></em>, <em class="sig-param"><span class="n">length</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#as_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.as_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert indexes into a binary mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indexes</strong> (<em>LongTensor</em>) – positive indexes</p></li>
<li><p><strong>length</strong> (<em>int</em>) – maximal possible value of indexes</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.one_hot">
<code class="sig-name descname">one_hot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">size</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#one_hot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand indexes into one-hot vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>Tensor</em>) – index</p></li>
<li><p><strong>size</strong> (<em>int</em>) – size of the one-hot dimension</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="torchdrug.layers.functional.multi_slice_mask">
<code class="sig-name descname">multi_slice_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">starts</span></em>, <em class="sig-param"><span class="n">ends</span></em>, <em class="sig-param"><span class="n">length</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#multi_slice_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.multi_slice_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the union of multiple slices into a binary mask.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_slice_mask</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>starts</strong> (<em>LongTensor</em>) – start indexes of slices</p></li>
<li><p><strong>ends</strong> (<em>LongTensor</em>) – end indexes of slices</p></li>
<li><p><strong>length</strong> (<em>int</em>) – length of mask</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="sampling">
<h3>Sampling<a class="headerlink" href="#sampling" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.multinomial">
<code class="sig-name descname">multinomial</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">num_sample</span></em>, <em class="sig-param"><span class="n">replacement</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#multinomial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Fast multinomial sampling. This is the default implementation in PyTorch v1.6.0+.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – unnormalized distribution</p></li>
<li><p><strong>num_sample</strong> (<em>int</em>) – number of samples</p></li>
<li><p><strong>replacement</strong> (<em>bool</em><em>, </em><em>optional</em>) – sample with replacement or not</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="activation">
<h3>Activation<a class="headerlink" href="#activation" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="torchdrug.layers.functional.shifted_softplus">
<code class="sig-name descname">shifted_softplus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchdrug/layers/functional/functional.html#shifted_softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchdrug.layers.functional.shifted_softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Shifted softplus function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<em>Tensor</em>) – input tensor</p>
</dd>
</dl>
</dd></dl>
</div>
</div>
</div>

      </article>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="models.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">torchdrug.models</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="metrics.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">torchdrug.metrics</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2021, MilaGraph Group
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="../_sources/api/layers.rst.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">torchdrug.layers</a><ul>
<li><a class="reference internal" href="#common-layers">Common Layers</a><ul>
<li><a class="reference internal" href="#gaussiansmearing">GaussianSmearing</a></li>
<li><a class="reference internal" href="#multilayerperceptron">MultiLayerPerceptron</a></li>
<li><a class="reference internal" href="#mutualinformation">MutualInformation</a></li>
<li><a class="reference internal" href="#pairnorm">PairNorm</a></li>
<li><a class="reference internal" href="#sequential">Sequential</a></li>
</ul>
</li>
<li><a class="reference internal" href="#convolution-layers">Convolution Layers</a><ul>
<li><a class="reference internal" href="#chebyshevconv">ChebyshevConv</a></li>
<li><a class="reference internal" href="#continuousfilterconv">ContinuousFilterConv</a></li>
<li><a class="reference internal" href="#graphattentionconv">GraphAttentionConv</a></li>
<li><a class="reference internal" href="#graphconv">GraphConv</a></li>
<li><a class="reference internal" href="#graphisomorphismconv">GraphIsomorphismConv</a></li>
<li><a class="reference internal" href="#messagepassing">MessagePassing</a></li>
<li><a class="reference internal" href="#neuralfingerprintconv">NeuralFingerprintConv</a></li>
<li><a class="reference internal" href="#relationalgraphconv">RelationalGraphConv</a></li>
</ul>
</li>
<li><a class="reference internal" href="#readout-layers">Readout Layers</a><ul>
<li><a class="reference internal" href="#meanreadout">MeanReadout</a></li>
<li><a class="reference internal" href="#sumreadout">SumReadout</a></li>
<li><a class="reference internal" href="#maxreadout">MaxReadout</a></li>
<li><a class="reference internal" href="#set2set">Set2Set</a></li>
<li><a class="reference internal" href="#softmax">Softmax</a></li>
<li><a class="reference internal" href="#sort">Sort</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pooling-layers">Pooling Layers</a><ul>
<li><a class="reference internal" href="#diffpool">DiffPool</a></li>
<li><a class="reference internal" href="#mincutpool">MinCutPool</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sampler-layers">Sampler Layers</a><ul>
<li><a class="reference internal" href="#edgesampler">EdgeSampler</a></li>
<li><a class="reference internal" href="#nodesampler">NodeSampler</a></li>
</ul>
</li>
<li><a class="reference internal" href="#flow-layers">Flow Layers</a><ul>
<li><a class="reference internal" href="#conditionalflow">ConditionalFlow</a></li>
</ul>
</li>
<li><a class="reference internal" href="#distribution-layers">Distribution Layers</a><ul>
<li><a class="reference internal" href="#independentgaussian">IndependentGaussian</a></li>
</ul>
</li>
<li><a class="reference internal" href="#functional-layers">Functional Layers</a><ul>
<li><a class="reference internal" href="#embedding-score-functions">Embedding Score Functions</a></li>
<li><a class="reference internal" href="#sparse-matrix-multiplication">Sparse Matrix Multiplication</a></li>
<li><a class="reference internal" href="#variadic">Variadic</a></li>
<li><a class="reference internal" href="#tensor-reduction">Tensor Reduction</a></li>
<li><a class="reference internal" href="#tensor-construction">Tensor Construction</a></li>
<li><a class="reference internal" href="#sampling">Sampling</a></li>
<li><a class="reference internal" href="#activation">Activation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
  </body>
</html>